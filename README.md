# AdaDepthCLIP: Adapter-based CLIP for Monocular Depth Estimation

This repository contains our experiments on **monocular depth estimation** using the **CLIP ViT-B/32 backbone**, **MLP adapters**, and a **composite loss function**.  
Our best configuration â€” *ViT-B/32 + Adapters + Composite Loss* â€” significantly improves prediction accuracy compared to baseline reproductions.

---

## ğŸš€ Key Features
- ğŸ”¥ **CLIP ViT-B/32 backbone** with partial fine-tuning  
- ğŸ§© **MLP Adapters** inserted at layers 2, 5, 8, 11  
- âš–ï¸ **Composite Loss**: combination of SILog, Gradient, SSIM, and L1  
- ğŸ§ª Support for **Mixture-of-Adapters (MoA)** with entropy regularization  
- ğŸ“ Easy training on **NYU Depth V2** dataset  
- ğŸ“Š TensorBoard logging for loss curves and gating probabilities  

---

## ğŸ“Š Results

### Performance Comparison

| Backbone + Adapters + Loss              | Î´1 â†‘  | Î´2 â†‘  | Î´3 â†‘  | AbsRel â†“ | Log10 â†“ | RMSE â†“ |
|-----------------------------------------|-------|-------|-------|----------|---------|--------|
| ResNet50 Backbone ğŸ§Š + MLP adaptor       | 0.390 | 0.680 | 0.848 | 0.393    | 0.158   | 1.176  |
| ResNet50 Backbone ğŸ”¥ + MLP adaptor       | 0.344 | 0.674 | 0.858 | 0.327    | 0.162   | 1.190  |
| ViT-B/32 Backbone ğŸ”¥ + MLP adaptors      | 0.394 | 0.681 | 0.854 | 0.408    | 0.155   | 1.150  |
| ViT-B/32 Backbone ğŸ”¥ + Adapters + Loss   | 0.417 | 0.701 | 0.890 | 0.377    | 0.147   | 1.096  |
| **ViT-B/32 + Adapters + Composite Loss**| **0.503** | **0.791** | **0.925** | **0.310** | **0.121** | **0.843** |

âœ… **Our best model outperforms all baselines.**

---

## âš™ï¸ Installation

```bash
git clone https://github.com/ReyhanehAhani/AdaDepthCLIP.git
cd AdaDepthCLIP
conda create -n adadepth python=3.10
conda activate adadepth
pip install -r requirements.txt
```

Dependencies include:
- `torch`, `torchvision`, `open_clip_torch`, `tqdm`, `tensorboard`, `PIL`, `numpy`

---

## ğŸ‹ï¸ Training

### Train baseline (ViT-B/32 + Adapters + Composite Loss)
```bash
python train_vit_b32_learnable_loss.py   --train_data_root ./nyu_images/train   --val_data_root ./nyu_images/test   --train_data_path ./datasets/nyudepthv2_train_files_with_gt_dense.txt   --val_data_path ./datasets/nyudepthv2_test_files_with_gt_dense.txt   --model_path ./checkpoints/monoclip_vit_b32   --log_dir ./tensorboard_logs/monoclip_vit_b32   --epochs 50 --batch_size 8
```

### Train with Mixture-of-Adapters (MoA v2)
```bash
bash run_train_vit_moa_v2.sh
```

---

## ğŸ” Evaluation
After training, evaluate the best model:
```bash
python eval.py --model_path ./checkpoints/monoclip_vit_b32/best_model.pth
```

Metrics include **AbsRel, Log10, RMSE, Î´1, Î´2, Î´3**.

---

## ğŸ“‚ Project Structure
```
AdaDepthCLIP/
â”‚
â”œâ”€â”€ monoclip_vit_b32.py          # ViT-B/32 + MLP adapters
â”œâ”€â”€ monoclip_vit_moa_v2.py       # Mixture-of-Adapters version
â”œâ”€â”€ train_vit_b32_learnable_loss.py
â”œâ”€â”€ train_vit_moa_v2.py
â”œâ”€â”€ run_train_vit_moa_v2.sh
â”œâ”€â”€ datasets/nyudepthv2_*.txt
â””â”€â”€ checkpoints/
```

---

## âœ¨ Citation
If you use this repo, please cite it as:
```bibtex
@misc{adadepth2025,
  title   = {AdaDepthCLIP: Adapter-based CLIP for Monocular Depth Estimation},
  author  = {Reyhaneh Ahani},
  year    = {2025},
  url     = {https://github.com/ReyhanehAhani/AdaDepthCLIP}
}
```

---

## ğŸ“ License
This project is released under the MIT License.
